{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcount example\n",
    "\n",
    "This notebook shows the classic wordcount example in which we want to calculate how many time the same word appears within a text.\n",
    "The example also show how to use basic PySpark tools like DataFrame and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out where the pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Wordcount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the step below we are going to read an input (local) file that will be our data source. textFile() and wholeTextFiles() methods to read into RDD that are the low level data access of Spark (there exist other method to read directly in Dataframe).\n",
    "\n",
    "Each line of the text file is a *row*. We can apply a series of chained operation:\n",
    "1. flatMap produces a new dataset <word> from the splitting\n",
    "2. map produces a new dataset in the form <word, couunt>\n",
    "3. reduceByKey coordinates the aggregation by summing rows with the same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating words count\n",
    "text_file = sc.textFile(\"doveconviene_info.txt\", numPartitions=10)\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print some basec statistics about words and their occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing each word with its respective count\n",
    "output = counts.collect()\n",
    "for (word, occurs) in output:\n",
    "    print(\"%s: %i\" % (word, occurs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe in PySpark\n",
    "\n",
    "PySpark is Python on Spark, programming language and syntax are the ones that we already know.\n",
    "\n",
    "*Dataframe* is the approach to analysis via high-level data structures that work in a distributed way. DataFrames is a data structure already known in Python/R that allow you to have a tabular/columnar form of the data. Operations are simple to execute and runs in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a friendly structure: dataframe\n",
    "https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row # import the pyspark sql Row class\n",
    "\n",
    "rows = counts.map(lambda p: Row(word=p[0], occurs=int(p[1]))) # tuples -> Rows\n",
    "#rows.toDF().createOrReplaceTempView(\"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(10,truncate= True) \n",
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns), df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe('occurs').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('occurs','word').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('word').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('word').distinct().count(), df.select('word').count(),df.select('word').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(df['occurs'].desc()).show(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to SQL\n",
    "\n",
    "We can also use SQL syntax to analyze Dataframe.We first need to define a virtual SQL table from the dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " word_count = df.createOrReplaceTempView(\"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from word_count order by occurs DESC ').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select                \n",
    "* \n",
    "from word_count\n",
    "WHERE length(word) > 2\n",
    "ORDER BY occurs  DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, save our result\n",
    "df.write.mode('overwrite').option(\"header\", \"true\").save('wordcount.csv',format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideration about Dataframe and SQL\n",
    "\n",
    "Dataframes are interchangeable with SQL. We can register a DataFrame as a SQL table and query it via SQL syntax.\n",
    "\n",
    "Benefits? High level access to the dataset. DataFrame as a SQL have similar performances. RDD (low level access) is fastest than Dataframe and SQL.\n",
    "More info here: \n",
    "https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547\n",
    "https://stackoverflow.com/questions/45430816/writing-sql-vs-using-dataframe-apis-in-spark-sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "### Dataframe\n",
    "\n",
    "Good operation overview.\n",
    "https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53\n",
    "\n",
    "The basic operations on Dataframe are described in terms operation requirements.\n",
    "https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "\n",
    "### SQL\n",
    "SparkSql follows Hive style, so you can refer to Hive Syntax for documentation.\n",
    "https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select\n",
    "\n",
    "The supported and unsupported Hive features by SparkSql can be found in the official documentation.\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#compatibility-with-apache-hive\n",
    "\n",
    "### SparkExamples\n",
    "\n",
    "Below a reference site for a good hoverview of basic Spark operations (cluster setup, RDD operations, Dataframe operations, SQL operations, Streaming, integration with other frameworks).\n",
    "It is for Scala but it will be easy get feedback also for PySpark.\n",
    "https://sparkbyexamples.com/![image.png](attachment:image.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
